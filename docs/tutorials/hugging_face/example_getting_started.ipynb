{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8add84",
   "metadata": {},
   "source": [
    "# Getting Started with Syne Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b7835",
   "metadata": {},
   "source": [
    "This notebook shows how to get started with Syne Tune. We will use a simple toy example here, for which tuning runs locally (and does not need a GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fc95b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install Syne Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'syne-tune'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dbc74b",
   "metadata": {},
   "source": [
    "This install a bare-bones version of Syne Tune, sufficient to run this notebook.\n",
    "* `pip install 'syne-tune'[gpsearchers]` installs a number of built-in Gaussian process HPO methods\n",
    "* `pip install 'syne-tune'[extra]` installs everything, including Ray (for Ray Tune optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1d985",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "For a start, we need a training script, as will be familiar to SageMaker users. We will use `train_height.py`. Tuning is enabled by reporting metric values back to Syne Tune. If training is iterative, you should report at the end of each iteration (e.g., epoch in neural network training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c341330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_height.py\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from syne_tune import Reporter\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.INFO)\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--steps', type=int)\n",
    "    parser.add_argument('--width', type=float)\n",
    "    parser.add_argument('--height', type=float)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    report = Reporter()\n",
    "\n",
    "    for step in range(args.steps):\n",
    "        mean_loss = (0.1 + args.width * step / 100) ** (-1) + args.height * 0.1\n",
    "        # Feed the score back to Syne Tune.\n",
    "        report(mean_loss=mean_loss, epoch=step + 1)\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1e668",
   "metadata": {},
   "source": [
    "## Run Tuning Locally\n",
    "\n",
    "This script comes with hyperparameters `width` and `height`, whereas `steps` is the maximum number of iterations. While the training script provides the function to be tuned, we need to define a domain for the search. This is done by a *configuration space*.\n",
    "\n",
    "For each hyperparameter, we define a type and range. In our example, both `width` and `height` are integers uniformly distributed between 1 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from syne_tune import Tuner, StoppingCriterion\n",
    "from syne_tune.backend import LocalBackend\n",
    "from syne_tune.config_space import randint\n",
    "from syne_tune.optimizer.baselines import ASHA\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# Configuration space\n",
    "config_space = {\n",
    "    'width': randint(1, 20),\n",
    "    'height': randint(1, 20),\n",
    "    'steps': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d145105",
   "metadata": {},
   "source": [
    "Next, we create the components required for tuning:\n",
    "* The trial backend executing the trainings. We use the local backend, which runs training jobs as subprocesses\n",
    "* The scheduler which contains the logic of the HPO algorithm. We use asynchronous successive halving\n",
    "* The tuner which coordinates these two, runs the experiments and records result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadce3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_backend = LocalBackend(entry_point='train_height.py')\n",
    "\n",
    "scheduler = ASHA(\n",
    "    config_space,\n",
    "    metric='mean_loss',  # name of metric to tune\n",
    "    mode='min',  # should be minimized\n",
    "    resource_attr='epoch',  # name of resource parameter (epochs in our example)\n",
    "    max_resource_attr='steps',  # config_space attribute for maximum number of epochs\n",
    "    search_options={'debug_log': False},  # suppress verbose log output\n",
    ")\n",
    "\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=StoppingCriterion(max_wallclock_time=45),  # run for 45 secs\n",
    "    n_workers=4,  # how many trials are evaluated in parallel\n",
    ")\n",
    "\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f668c7a",
   "metadata": {},
   "source": [
    "## Plot Results\n",
    "\n",
    "The results of an experiment are stored to a dataframe, which contains all metrics received (with time stamps) and their configurations. Based on this data, we can plot the best value found over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d78b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syne_tune.experiments import load_experiment\n",
    "\n",
    "tuning_experiment = load_experiment(tuner.name)\n",
    "tuning_experiment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc2f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
