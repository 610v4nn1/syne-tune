{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with Population-based Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will use Population-based training (PBT) to optimize the learning rate and weight decay for fine-tuning our classifier based on SageMakerTune.\n",
    "\n",
    "**Note**: This notebook demonstrates the **remote tuning** feature of SageMaker Tune, meaning that the HPO experiment with all its training jobs is executed remotely as a SageMaker training job. Advantages of remote tuning include:\n",
    "\n",
    "* You can run this notebook on your local machine (as long as it can launch SageMaker training jobs), or on a notebook\n",
    "  instance without GPUs\n",
    "* You can start many experiments in parallel, they'll be executed as different SageMaker training jobs\n",
    "* Your training code can use a SageMaker framework, you don't have to build Docker containers (in the example here, we\n",
    "  will use the **HuggingFace** framework)\n",
    "* Experiment logs and results are archived as for any SageMaker training jobs, result files are written to S3\n",
    "\n",
    "On the other hand, remote tuning can be a bit clunky for debugging. Results are not immediately available as local files, for example. With SageMaker Tune, you can pick and choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SageMakerTune\n",
    "\n",
    "First we need to install SagemakerTune. The command below assumes you are on a SageMaker notebook instance, where the SageMaker Tune sources are checked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/develop/sagemaker-tune\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.3)\n",
      "Obtaining file:///home/ec2-user/develop/sagemaker-tune\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (1.18.29)\n",
      "Requirement already satisfied: sagemaker>=2.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (2.55.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (0.3.4)\n",
      "Requirement already satisfied: PyYaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (5.3.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (1.0.1)\n",
      "Requirement already satisfied: ujson in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (1.35)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (3.10.0.0)\n",
      "Requirement already satisfied: pytest in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (5.3.5)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (1.4.1)\n",
      "Requirement already satisfied: autograd in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (1.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker-tune==0.1) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (1.18.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (0.2.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (20.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (1.5.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (0.1.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (3.11.4)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (0.2.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=2.32.0->sagemaker-tune==0.1) (19.3.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->sagemaker-tune==0.1) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->sagemaker-tune==0.1) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.29 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->sagemaker-tune==0.1) (1.21.29)\n",
      "Requirement already satisfied: future>=0.15.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from autograd->sagemaker-tune==0.1) (0.18.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->sagemaker-tune==0.1) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->sagemaker-tune==0.1) (2.8.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->sagemaker-tune==0.1) (1.8.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->sagemaker-tune==0.1) (8.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->sagemaker-tune==0.1) (0.13.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pytest->sagemaker-tune==0.1) (0.1.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->sagemaker-tune==0.1) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->sagemaker-tune==0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->sagemaker-tune==0.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->sagemaker-tune==0.1) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.32.0->sagemaker-tune==0.1) (2.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker>=2.32.0->sagemaker-tune==0.1) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker>=2.32.0->sagemaker-tune==0.1) (2.4.6)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.32.0->sagemaker-tune==0.1) (45.2.0.post20200210)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker>=2.32.0->sagemaker-tune==0.1) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker>=2.32.0->sagemaker-tune==0.1) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker>=2.32.0->sagemaker-tune==0.1) (1.6.6.4)\n",
      "Installing collected packages: sagemaker-tune\n",
      "  Attempting uninstall: sagemaker-tune\n",
      "    Found existing installation: sagemaker-tune 0.1\n",
      "    Uninstalling sagemaker-tune-0.1:\n",
      "      Successfully uninstalled sagemaker-tune-0.1\n",
      "  Running setup.py develop for sagemaker-tune\n",
      "Successfully installed sagemaker-tune-0.1\n",
      "/home/ec2-user/develop/sagemaker-tune/examples/hugging_face_example\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/develop/sagemaker-tune/\n",
    "! pip install --upgrade pip\n",
    "! pip install -e .[gpsearchers]\n",
    "%cd examples/hugging_face_example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Optimization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies the maximum number of parallel workers to evaluate hyperparameter configurations. Training jobs of different\n",
    "# workers are executed as subprocesses (we are using the \"local\" back-end). This means you should choose the number of\n",
    "# workers to be no larger than the number of GPUs on the instance.\n",
    "# If you like to use more workers or run on smaller GPU instances, you should use the \"sagemaker\" back-end, which\n",
    "# however comes with larger overheads for starting and stopping a training job.\n",
    "n_workers = 4\n",
    "\n",
    "# The size of the population that PBT maintains\n",
    "population_size = 8\n",
    "\n",
    "# Specifies whether we maximize or minimize our metric\n",
    "mode = 'max'\n",
    "\n",
    "# The metric we aim to optimize. Needs to match the key name reported in your training script\n",
    "metric = 'accuracy'\n",
    "\n",
    "# PBT assignes more resources to better performing models. This parameters defines the resources that we want to use\n",
    "resource_attr = 'iteration'\n",
    "\n",
    "# The maximum amount of resources we can assign to any model.\n",
    "max_iterations = 73 # corresponds to 3 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Configuration Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the domain for each hyperparameter we want to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_tune.search_space import loguniform\n",
    "\n",
    "config_space = {\n",
    "    'learning_rate': loguniform(1e-6, 1e-4),\n",
    "    'weight_decay': loguniform(1e-6, 1e-4)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have additional arguments that we want pass to our train function, we can add them to the config space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space['model_name'] = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hugging Face SageMaker Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use SagemakerTune we need to modify our train function slightly. First we will add an additional\n",
    "command line argument to specify the hyperparameters. Additionally we will add an argument that specifies where we will store the checkpoints. This will parameter will be automatically set by SagemakerTune and allows is to start the evaluation of a configuration from a previous configuration\n",
    "\n",
    "\n",
    "```\n",
    "from sagemaker_tune.constants import SMT_CHECKPOINT_DIR\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-5)\n",
    "    parser.add_argument(\"--eval_steps\", type=int, default=32)\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default='./output')\n",
    "\n",
    "    parser.add_argument(f\"--{SMT_CHECKPOINT_DIR}\", type=str)\n",
    "\n",
    "```\n",
    "\n",
    "Next we need to report the performance of the current model back to SageMakerTune. For that we will\n",
    "add a callback function, which reports results back to SageMaker Tune after each evaluation:\n",
    "\n",
    "```\n",
    "    from sagemaker_tune.report import Reporter\n",
    "\n",
    "    report = Reporter()\n",
    "\n",
    "    class Callback(TrainerCallback):\n",
    "        def __init__(self):\n",
    "            self.iteration = 1\n",
    "\n",
    "        def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "            # Feed the validation accuracy back to Tune\n",
    "            report(iteration=self.iteration, accuracy=metrics['eval_accuracy'])\n",
    "            self.iteration += 1\n",
    "\n",
    "\n",
    "    trainer.add_callback(Callback())\n",
    "```    \n",
    "\n",
    "Lastly, we need to make sure that if SageMakerTune provides us with a checkpoint, we start the evaluation from there:\n",
    "\n",
    "``` \n",
    "    if os.listdir(checkpoint_dir) == []:\n",
    "        trainer.train()\n",
    "    else:\n",
    "        trainer.train(resume_from_checkpoint=os.path.join(checkpoint_dir, os.listdir(checkpoint_dir)[0]))\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sagemaker_tune\n",
    "\n",
    "p = Path.cwd()\n",
    "training_script = str(p) +  '/scripts/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the LocalBackend which distributed the HPO on the local machine. Alternatively, we can also use the SageMakerBackend where each trial, i.e function evaluation will be executed on a Sagemaker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_tune.backend.local_backend import LocalBackend\n",
    "    \n",
    "backend = LocalBackend(entry_point=training_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining PBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker_tune.optimizer.schedulers.searchers.utils.default_arguments:scheduler_options: Key 'searcher': Imputing default value random\n",
      "scheduler_options: Key 'resume': Imputing default value False\n",
      "\n",
      "INFO:sagemaker_tune.optimizer.schedulers.fifo:Master random_seed = 354364674\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_tune.optimizer.schedulers.pbt import PopulationBasedTraining\n",
    "\n",
    "scheduler =  PopulationBasedTraining(config_space=config_space,\n",
    "                                  metric=metric,\n",
    "                                  resource_attr=resource_attr,\n",
    "                                  population_size=population_size,\n",
    "                                  mode=mode,\n",
    "                                  max_t=max_iterations,\n",
    "                                  perturbation_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote Tuning\n",
    "\n",
    "We are now ready to start tuning. As noted above, we will use **remote tuning**: the experiment will be run on a suitable instance, instead of locally on the machine this notebook is running. Results will be written to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker_tune.tuner:results of trials will be saved on /home/ec2-user/sagemaker-tune/pbt-example-2021-10-22-13-41-19-605\n",
      "INFO:sagemaker_tune.backend.sagemaker_backend.sagemaker_utils:No Sagemaker role passed as environment variable $AWS_ROLE, inferring it.\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment launched as pbt-example-2021-10-22-13-41-19-605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:copy endpoint files from /home/ec2-user/develop/sagemaker-tune/examples/hugging_face_example/scripts to /home/ec2-user/develop/sagemaker-tune/tuner\n",
      "INFO:root:copy endpoint script requirements to /home/ec2-user/develop/sagemaker-tune/sagemaker_tune/remote\n",
      "INFO:root:Tuner will checkpoint results to s3://sagemaker-us-west-2-719355911555/sagemaker-tune/pbt-example-2021-10-22-13-41-19-605/\n",
      "INFO:root:Fetching sagemaker tune image 719355911555.dkr.ecr.us-west-2.amazonaws.com/smt-cpu-py36\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: pbt-example-2021-10-22-13-41-19-605\n"
     ]
    }
   ],
   "source": [
    "from sagemaker_tune.tuner import Tuner\n",
    "from sagemaker_tune.stopping_criterion import StoppingCriterion\n",
    "from sagemaker_tune.remote.remote_launcher import RemoteLauncher\n",
    "\n",
    "# Tuning will run for 3 hours (which won't give us competitive results, but it's just a demo)\n",
    "scheduler_timeout = 3 * 3600\n",
    "stop_criterion = StoppingCriterion(max_wallclock_time=scheduler_timeout)\n",
    "\n",
    "# Tuner, as run on the remote instance\n",
    "local_tuner = Tuner(\n",
    "    backend=backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=n_workers,\n",
    "    tuner_name='pbt-example')\n",
    "\n",
    "print(f\"Experiment launched as {local_tuner.name}\")\n",
    "\n",
    "# The experiment is run as SageMaker estimator, we need to set its parameters\n",
    "estimator_kwargs = {\n",
    "    'framework_version': '4.4',\n",
    "    'pytorch_version': '1.6',\n",
    "    'max_run': int(1.01 * scheduler_timeout),\n",
    "}\n",
    "\n",
    "dependencies = [str(Path.cwd().parent.parent / \"benchmarks/\")]\n",
    "tuner = RemoteLauncher(\n",
    "    tuner=local_tuner,\n",
    "    dependencies=dependencies,\n",
    "    instance_type='ml.g4dn.12xlarge',\n",
    "    framework='HuggingFace',\n",
    "    estimator_kwargs=estimator_kwargs,\n",
    "    no_tuner_logging=True)\n",
    "\n",
    "results = tuner.run(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "TODO! Have to load results from S3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the optimization process of PBT. Each dot represents the performance of \n",
    "the current model and the color indicates the same hyperparameter configuration. The solid blue line represents the \n",
    "performance of the best model found so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sagemaker_tune.experiments import load_experiment\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "r = load_experiment(tuner.tuner_path)\n",
    "df = r.results\n",
    "plt.plot(df['smt_tuner_time'], df['accuracy'].cummax(), linewidth=4)\n",
    "\n",
    "for trial_id in df.trial_id.unique():\n",
    "    sub_df = df[df['trial_id'] == trial_id]\n",
    "    y = sub_df['accuracy'].cummax()\n",
    "    rt =sub_df['smt_tuner_time']    \n",
    "    plt.plot(rt, y, marker='o', alpha=0.4)\n",
    "\n",
    "plt.ylabel('validation accuracy')\n",
    "plt.xlabel('wall-clock time (seconds)')\n",
    "plt.ylim(0.8, 0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
